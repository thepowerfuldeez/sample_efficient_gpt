base: gpt_small_faster

data_mix: &data_mix
  tokenizer_path: HuggingFaceTB/SmolLM2-135M
  validation_path: /home/george/datasets/val.npy
  train_path: 1226_stable.json
  context_length: 512
  batch_size: 8
  val_batch_size: 4

trainer_base: &trainer_base
  dist_mode: ddp
  save_dir: /mnt/harddrive/checkpoints
  log_every: 1
  val_every: 1000000000
  save_every: 1000000000
  run_name: "{date}_moe_smoke"
  gradient_accumulation_steps: 1
  max_steps: 200

optim_base: &optim_base
  scheduler: wsd
  lr: 3e-4
  warmup_steps: 50
  wsd_need_warmup: true
  wsd_phase: stable

model_small_moe: &model_small_moe
  vocab_size: 49152
  d_model: 512
  d_ff: 2048
  n_layers: 12
  n_heads: 8
  n_kv_heads: 2
  weight_tying: true
  layernorm_scaling: true
  attn_qknorm: true
  attn_gating: false
  attn_val_residual: false
  moe_num_experts: 8
  moe_top_k: 1
  moe_capacity_factor: 1.25
  moe_aux_loss_coef: 0.01
  moe_z_loss_coef: 0.001
  moe_router_jitter: 0.05
  moe_normalize_gates: true
  moe_start_layer: 2
  moe_every_n_layers: 2
  moe_end_layer: null

experiments:
  moe_smoke_1gpu:
    model: *model_small_moe
    trainer: *trainer_base
    optim: *optim_base
    data: *data_mix

  moe_ep_smoke_2gpu:
    model:
      <<: *model_small_moe
      moe_num_experts: 4
      moe_expert_parallel_size: 2
    trainer:
      <<: *trainer_base
      run_name: "{date}_moe_ep_smoke"
    optim: *optim_base
    data:
      <<: *data_mix
      batch_size: 16
      
  smollm2_moe49:
    model:
      vocab_size: 49152
      d_model: 576
      d_ff: 1536
      n_layers: 30
      n_heads: 9
      n_kv_heads: 3
      theta: 100000
      weight_tying: true
      layernorm_scaling: true
      attn_qknorm: true
      attn_gating: per-head
      attn_val_residual: true
      moe_gate_scale: 5.0
      moe_num_experts: 49
      moe_num_shared_experts: 1
      moe_top_k: 1
      moe_capacity_factor: 1.00
      moe_aux_loss_coef: 0.001
      moe_router_jitter: 0.001
      moe_start_layer: 2
      moe_every_n_layers: 1
      moe_end_layer: 30
    trainer:
      dist_mode: ddp
      save_dir: /home/george/checkpoints
      run_name: "{date}_smollm2_moe49"
      log_every: 50
      val_every: 1000
      save_every: 1000
      gradient_accumulation_steps: 1
      max_steps: 10000
      compile: true
      load_from: /home/george/checkpoints/smollm_moe_49ep.pt
      # load_from: /root/checkpoints/1218_smollm2_moe48_ep_fp8_b200/1000.rank0.pt
    optim:
      scheduler: wsd
      muon_wd: 0.1
      muon_lr: 6e-3
      lr: 4e-3
      warmup_steps: 1000
      wsd_need_warmup: true
      wsd_phase: stable
      zero_lr_steps: 100
    data:
      tokenizer_path: HuggingFaceTB/SmolLM2-135M
      validation_path: /home/george/datasets/val.npy
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/1226_stable.json
      context_length: 768
      batch_size: 768
      val_batch_size: 32
