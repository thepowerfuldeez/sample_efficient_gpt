base: gpt_small_faster
model_main: &model_main
  vocab_size: 65536
  attn_qknorm: true
  attn_gating: per-head
  d_model: 1280
  d_ff: 3392
  n_layers: 18
  n_heads: 20

trainer_base: &trainer_base
  dist_mode: ddp
  save_dir: /mnt/harddrive/checkpoints
  val_every: 5000
  run_name: "{date}_main_run_mix"

optim_base: &optim_base
  scheduler: wsd
  muon_wd: 0.1
  muon_lr: 1.44e-2
  warmup_steps: 20000
  wsd_need_warmup: true
  wsd_phase: stable

data_mix: &data_mix
  tokenizer_path: tokenizer/trained_tokenizers/mix_bpe_hf
  validation_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_dclm_edu/tokenized_65536/val.npy
  train_path: 1031_data_mix.json
  context_length: 704
  batch_size: 70
  val_batch_size: 16

base_experiment: &base_experiment
  model: *model_main
  trainer: *trainer_base
  optim: *optim_base
  data: *data_mix

experiments:
  main_run_mix:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix"
      gradient_accumulation_steps: 8
      max_steps: 1400000
      save_every: 10000
      load_from: /mnt/harddrive/checkpoints/1111_main_run_mix/1250000.pt
    optim:
      <<: *optim_base
      muon_lr: 1.44e-2
    data:
      <<: *data_mix

  main_run_mix_stable_continue:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix"
      gradient_accumulation_steps: 1
      max_steps: 42500
      save_every: 2500
      val_every: 1000
      # load_from: /root/imu1_base_ema.pt
      load_from: /root/checkpoints/1202_main_run_mix/2500.pt
      save_dir: /root/checkpoints/
    optim:
      <<: *optim_base
      muon_lr: 9.7e-3
      lr: 4.85e-3
      wsd_need_warmup: true
      wsd_phase: stable
      warmup_steps: 1000
    data:
      <<: *data_mix
      tokenizer_path: /root/.cache/huggingface/hub/models--thepowerfuldeez--imu_1_base/snapshots/f37099bbd98d1a33808ab59a71d31721da8bb543/
      validation_path: /root/val.npy
      train_path: /root/sample_efficient_gpt/sample_efficient_gpt/1202_main_run_data.json
      context_length: 832
      batch_size: 1024
      val_batch_size: 16

  main_run_mix_stable_continue2:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_continue"
      gradient_accumulation_steps: 1
      max_steps: 40000
      save_every: 2500
      val_every: 1000
      load_from: /root/1300000.pt
      save_dir: /root/checkpoints/
    optim:
      <<: *optim_base
      muon_lr: 1.44e-2
      wsd_need_warmup: false
      wsd_phase: stable
    data:
      <<: *data_mix
      tokenizer_path: /root/.cache/huggingface/hub/models--thepowerfuldeez--imu_1_base/snapshots/f37099bbd98d1a33808ab59a71d31721da8bb543/
      validation_path: /root/val.npy
      train_path: /root/sample_efficient_gpt/sample_efficient_gpt/1204_main_run_data.json
      context_length: 704
      batch_size: 640
      val_batch_size: 16

  main_run_mix_decay:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_decay"
      gradient_accumulation_steps: 10
      max_steps: 1750000
      save_every: 5000
      load_from: /mnt/harddrive/checkpoints/1117_main_run_mix_decay/1665000.pt
    optim:
      <<: *optim_base
      muon_lr: 1.32e-2
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 1300000
    data:
      tokenizer_path: tokenizer/trained_tokenizers/mix_bpe_hf
      validation_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_dclm_edu/tokenized_65536/val.npy
      train_path: 1112_data_mix_stage2.json
      context_length: 1024
      batch_size: 50
      val_batch_size: 12

  main_run_mix_new_decay:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_decay"
      gradient_accumulation_steps: 1
      max_steps: 1345000
      save_every: 500
      val_every: 1000
      load_from: /root/1300000.pt
      save_dir: /root/checkpoints/
    optim:
      <<: *optim_base
      muon_lr: 3e-3
      lr: 1e-3
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 1300000
    data:
      <<: *data_mix
      tokenizer_path: thepowerfuldeez/imu_1_base
      validation_path: /root/val.npy
      train_path: /root/sample_efficient_gpt/sample_efficient_gpt/1202_main_run_data_decay.json
      context_length: 1152
      batch_size: 768
      val_batch_size: 16
      seed: 43

  main_run_mix_new_decay2:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_continue_decay"
      gradient_accumulation_steps: 1
      max_steps: 105000
      save_every: 1000
      val_every: 1000
      load_from: /root/checkpoints/1204_main_run_mix/40000.pt
      save_dir: /root/checkpoints/
    optim:
      <<: *optim_base
      muon_lr: 1.32e-2
      lr: 6.4e-3
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 40000
    data:
      <<: *data_mix
      tokenizer_path: /root/.cache/huggingface/hub/models--thepowerfuldeez--imu_1_base/snapshots/f37099bbd98d1a33808ab59a71d31721da8bb543/
      validation_path: /root/val.npy
      train_path: /root/sample_efficient_gpt/sample_efficient_gpt/1202_main_run_data_decay.json
      context_length: 1152
      batch_size: 480
      val_batch_size: 16

  main_run_mix_midtrain_stable:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_midtrain"
      gradient_accumulation_steps: 10
      max_steps: 37500
      save_every: 5000
      val_every: 2500
      load_from: /mnt/harddrive/checkpoints/1118_main_run_mix_midtrain/20000.pt
    optim:
      scheduler: wsd
      muon_wd: 0.1
      muon_lr: 1.32e-3
      warmup_steps: 2000
      wsd_need_warmup: true
      wsd_phase: stable
    data:
      tokenizer_path: tokenizer/trained_tokenizers/mix_bpe_hf2
      validation_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/val.npy
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/train
      context_length: 1024
      batch_size: 40
      val_batch_size: 10

  main_run_mix_midtrain_decay:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_midtrain"
      gradient_accumulation_steps: 10
      max_steps: 50000
      save_every: 5000
      val_every: 2500
      load_from: /mnt/harddrive/checkpoints/1121_main_run_mix_midtrain/37500.pt
    optim:
      scheduler: wsd
      muon_wd: 0.1
      muon_lr: 1.32e-3
      warmup_steps: 2000
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 37500
    data:
      tokenizer_path: tokenizer/trained_tokenizers/mix_bpe_hf2
      validation_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/val.npy
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/train
      context_length: 1024
      batch_size: 40
      val_batch_size: 10
