base: gpt_small_faster
model_main: &model_main
  vocab_size: 49152
  attn_qknorm: true
  attn_gating: per-head
  attn_val_residual: true
  d_model: 576
  d_ff: 1536
  n_layers: 30
  n_heads: 9
  n_kv_heads: 3
  weight_tying: true
  layernorm_scaling: true

trainer_base: &trainer_base
  dist_mode: ddp
  save_dir: /home/george/checkpoints
  val_every: 5000
  run_name: "{date}_main_run_mix"

optim_base: &optim_base
  scheduler: wsd
  muon_wd: 0.1
  muon_lr: 1.1e-2
  lr: 6e-3
  warmup_steps: 20000
  wsd_need_warmup: true
  wsd_phase: stable

data_mix: &data_mix
  tokenizer_path: HuggingFaceTB/SmolLM2-135M
  validation_path: /home/george/datasets/val.npy
  train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/1226_stable.json
  context_length: 768
  batch_size: 576
  val_batch_size: 32

base_experiment: &base_experiment
  model: *model_main
  trainer: *trainer_base
  optim: *optim_base
  data: *data_mix

experiments:
  stable:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_smollm_imu_135m_stable"
      gradient_accumulation_steps: 1
      max_steps: 40000
      save_every: 2500
      # load_from: /mnt/harddrive/checkpoints/1215_smollm_conversion/smollm2_imu_wide2x_gqa_extras_qknormrms_g8.pt
      load_from: /home/george/checkpoints/smollm_imu.pt
    optim:
      <<: *optim_base
      muon_lr: 7.5e-3
      lr: 4.8e-3
      zero_lr_steps: 500
      warmup_steps: 2500

  decay:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_smollm_imu_135m_decay"
      gradient_accumulation_steps: 1
      max_steps: 40000
      save_every: 2500
      load_from: /home/george/checkpoints/0103_smollm_imu_135m_stable/40000.pt
    optim:
      <<: *optim_base
      lr_min_coeff: 0.33
      muon_lr: 8e-3
      lr: 5e-3
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 40000
    data:
      <<: *data_mix
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/1227_decay.json
      context_length: 1024
      batch_size: 448
      seed: 43

  midtrain:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_smollm_imu_midtrain"
      gradient_accumulation_steps: 2
      max_steps: 265000
      save_every: 2500
      load_from: /home/george/checkpoints/1227_smollm_imu_decay/200000.pt
    optim:
      <<: *optim_base
      lr_min_coeff: 0.33
      muon_lr: 0.003
      lr: 0.002
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 200000
    data:
      <<: *data_mix
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/0101_midtrain.json
      context_length: 1152
      batch_size: 192
      val_batch_size: 8
      seed: 43

  main_run_mix_midtrain_stable:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_midtrain"
      gradient_accumulation_steps: 10
      max_steps: 37500
      save_every: 5000
      val_every: 2500
      load_from: /mnt/harddrive/checkpoints/1118_main_run_mix_midtrain/20000.pt
    optim:
      scheduler: wsd
      muon_wd: 0.1
      muon_lr: 1.32e-3
      warmup_steps: 2000
      wsd_need_warmup: true
      wsd_phase: stable
    data:
      tokenizer_path: tokenizer/trained_tokenizers/mix_bpe_hf2
      validation_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/val.npy
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/train
      context_length: 1024
      batch_size: 40
      val_batch_size: 10

  main_run_mix_midtrain_decay:
    <<: *base_experiment
    trainer:
      <<: *trainer_base
      run_name: "{date}_main_run_mix_midtrain"
      gradient_accumulation_steps: 10
      max_steps: 50000
      save_every: 5000
      val_every: 2500
      load_from: /mnt/harddrive/checkpoints/1121_main_run_mix_midtrain/37500.pt
    optim:
      scheduler: wsd
      muon_wd: 0.1
      muon_lr: 1.32e-3
      warmup_steps: 2000
      wsd_need_warmup: false
      wsd_phase: decay
      wsd_decay_step: 37500
    data:
      tokenizer_path: tokenizer/trained_tokenizers/mix_bpe_hf2
      validation_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/val.npy
      train_path: /home/george/sample_efficient_gpt/sample_efficient_gpt/data_midtrain/train
      context_length: 1024
      batch_size: 40
      val_batch_size: 10
